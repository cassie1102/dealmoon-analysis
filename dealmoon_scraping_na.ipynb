{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def scraper(pages):\n",
    "    '''\n",
    "    An automatic scrapping function to scrape the newest posts on dealmoon.com website. \n",
    "    -------------------------------------------------------------\n",
    "    Output: An Excel table. \n",
    "    '''\n",
    "    \n",
    "    result = pd.DataFrame(columns=('title','item1','item2','item3', 'comment_num', 'like_num', 'share_num', 'time', 'link'))\n",
    "    hdr = { 'User-Agent' : 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)' }\n",
    "    \n",
    "    for page in range(pages):\n",
    "        url = 'https://www.dealmoon.com/?p=%d' %(page)\n",
    "        req = urllib.request.Request(url, headers=hdr)\n",
    "        response = urllib.request.urlopen(req)\n",
    "        soup = BeautifulSoup(response.read(), 'html.parser')\n",
    "\n",
    "        for article in soup.find_all('div', {'class':'right-cnt'}):\n",
    "            try:\n",
    "                title = article.find('span', class_ = 'txt').text\n",
    "                link = article.find('a').get('href')\n",
    "                zoom = article.find()\n",
    "                dpc = article.find('div', class_ = 'dpc j-dpc')\n",
    "                sample_items = [item.get_text(strip = True).replace('\\xa0', '') \n",
    "                                for item in dpc.find_all('p', class_ = 'deal_text')]\n",
    "                j_nums = [num.get_text(strip = True) for num in article.find_all('em', class_ = 'j-count')]\n",
    "                time = article.find('span', class_ = 'ib published-date').get_text()\n",
    "\n",
    "                item1, item2, item3 = (sample_items)\n",
    "                num1, num2, num3 = (j_nums)\n",
    "\n",
    "                result = result.append(pd.DataFrame({'title':[title],'item1':[item1],'item2':[item2], 'item3':[item3], \n",
    "                                                    'comment_num':[num1], 'like_num':[num2], 'share_num':[num3], \n",
    "                                                     'time':[time], 'link':[link]}),\n",
    "                                       ignore_index=True)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if not page % 10:\n",
    "            print('Page %d is done.'%(page))\n",
    "            \n",
    "    from datetime import date\n",
    "    today = date.today()\n",
    "    result.to_excel('result/dealmoon_na_result_{}.xlsx'.format((today)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 is done.\n",
      "Page 10 is done.\n",
      "Page 20 is done.\n",
      "Page 30 is done.\n",
      "Page 40 is done.\n",
      "Page 50 is done.\n",
      "Page 60 is done.\n",
      "Page 70 is done.\n",
      "Page 80 is done.\n",
      "Page 90 is done.\n"
     ]
    }
   ],
   "source": [
    "scraper(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def beauty_scraper(pages):\n",
    "    '''\n",
    "    An automatic scrapping function to scrape the newest posts on dealmoon.com website-beauty section. \n",
    "    -------------------------------------------------------------\n",
    "    Output: An Excel table. \n",
    "    '''\n",
    "    \n",
    "    result = pd.DataFrame(columns=('title','item1','item2','item3', 'comment_num', 'like_num', 'share_num', 'time', 'link'))\n",
    "    hdr = { 'User-Agent' : 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)' }\n",
    "    \n",
    "    for page in range(pages):\n",
    "        url = 'https://www.dealmoon.com/beauty/?%d' %(page)\n",
    "        req = urllib.request.Request(url, headers=hdr)\n",
    "        response = urllib.request.urlopen(req)\n",
    "        soup = BeautifulSoup(response.read(), 'html.parser')\n",
    "\n",
    "        for article in soup.find_all('div', {'class':'right-cnt'}):\n",
    "            try:\n",
    "                title = article.find('span', class_ = 'txt').text\n",
    "                link = article.find('a').get('href')\n",
    "                zoom = article.find()\n",
    "                dpc = article.find('div', class_ = 'dpc j-dpc')\n",
    "                sample_items = [item.get_text(strip = True).replace('\\xa0', '') \n",
    "                                for item in dpc.find_all('p', class_ = 'deal_text')]\n",
    "                j_nums = [num.get_text(strip = True) for num in article.find_all('em', class_ = 'j-count')]\n",
    "                time = article.find('span', class_ = 'ib published-date').get_text()\n",
    "\n",
    "                item1, item2, item3 = (sample_items)\n",
    "                num1, num2, num3 = (j_nums)\n",
    "\n",
    "                result = result.append(pd.DataFrame({'title':[title],'item1':[item1],'item2':[item2], 'item3':[item3], \n",
    "                                                    'comment_num':[num1], 'like_num':[num2], 'share_num':[num3], \n",
    "                                                     'time':[time], 'link':[link]}),\n",
    "                                       ignore_index=True)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if not page % 10:\n",
    "            print('Page %d is done.'%(page))\n",
    "            \n",
    "    from datetime import date\n",
    "    today = date.today()\n",
    "    result.to_excel('result/na_beauty_result_{}.xlsx'.format((today)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 is done.\n",
      "Page 10 is done.\n",
      "Page 20 is done.\n",
      "Page 30 is done.\n",
      "Page 40 is done.\n",
      "Page 50 is done.\n",
      "Page 60 is done.\n",
      "Page 70 is done.\n",
      "Page 80 is done.\n",
      "Page 90 is done.\n"
     ]
    }
   ],
   "source": [
    "beauty_scraper(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
